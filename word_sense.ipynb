{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How the experiment works\n",
    "We used python3 as our programming language to implement the paper. We also used ***nltk*** package which is a widely used python package to work with natural language. We used ***WordNet*** which is a lexical database for the English language.\n",
    "We used four different types of word embedding models, which are:\n",
    "\n",
    " 1. [Glove](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    " 2. [Dependency-Based](http://u.cs.biu.ac.il/~yogo/data/syntemb/deps.words.bz2)\n",
    " 3. [Bag of Words (k = 2)](http://u.cs.biu.ac.il/~yogo/data/syntemb/bow2.words.bz2)\n",
    " 4. [Bag of Words (k = 5)](http://u.cs.biu.ac.il/~yogo/data/syntemb/bow5.words.bz2)\n",
    "\n",
    "Now, before we start to describe how our experiment works we need to describe some terms of our implementation. \n",
    "\n",
    "### Synset\n",
    "> \"Synonyms are words that have similar meanings. A synonym set, or **synset**, is a group of synonyms. A synset, therefore, corresponds to an abstract concept.\"\n",
    "\n",
    "#### Synset definition\n",
    "> \"Any word in the wordnet has some definitions associated with the word. A word can have multiple definitions which means every word can have multiple synsets and every synset has its own definition\"\n",
    "\n",
    "#### Word tokenizer\n",
    "> A word tokenizer splits word from the definition sentence of a synset and keeps those words in an array\n",
    "\n",
    "#### Pos tagger\n",
    "> pos-tagger/parts of speech tagger tags every word with respective parts of speech. For example, Noun, Verb, Adjective, etc\n",
    "\n",
    "#### Pos tagger\n",
    "> pos-tagger/parts of speech tagger tags every word with respective parts of speech. For example, Noun, Verb, Adjective, etc\n",
    "\n",
    "#### Cosine similarity\n",
    " >Cosine similarity is a measure of similarity between two non-zero vectors (in our case it is a word) of an inner product space that measures the cosine of the angle between them.\n",
    "#### WSD- word sense disambiguation\n",
    "> **word-sense disambiguation** (**WSD**)  concerned with identifying which sense of a word is used in a sentence.\n",
    "#### Lemma\n",
    "> The term ***Synset*** stands for \"set of synonyms\". A set of synonyms is a set of words with similar meaning, e.g. _ship, skiff, canoe, kayak_ might all be synonyms for _boat_. In the nltk, a ***Synset*** is, in fact, a set of ***lemmas*** with related meaning.\n",
    "\n",
    "## Description of the algorithm\n",
    "#### The steps can be described as followings:\n",
    " 1. The code first reads the word embedding from the file and stores it as a dictionary. The key of the dictionary is word and value is numpy array\n",
    " 2. From the input dataset, we read every line from the text file and we run our code for each of the lines. \n",
    " 3. For each line, we get a list of words with their respective parts of speech (**POS**). Now for every allowed **POS**(Noun, Verb, Adjective, Adverb) we find the associated vector/numpy array from our word embedding and store it in a dictionary. We also make a separate list for these chosen words only. \n",
    " 4. Now from this dictionary, we get word as key and values are vectors. We call this word as ***candidate word*** and vector as ***candidate vector***. For every candidate word, we find a list of ***Lemma/Sense***.   For every ***Lemma/Sense*** we find it's **key** and ***definition/glossary*** sentences.\n",
    " 5.  Now the ***definition/glossary*** of each sense are sentences. We repeat the steps to get words and it's respective parts of speech using ***pos-tagger*** again.  (We take only the Noun, Verb, Adjective, Adverb as per the paper). For every word we again get the word vector from the word embedding dictionary we calculate ***cosine similarity*** with ***candidate vector***. If the cosine similarity is bigger than a threshold we store the vector/array in a list. Lastly, we calculate the average of the vector for every Sense of the word and store the value in a dictionary. So, here the key is the Sense and value is the average of the vector/array. \n",
    " 6. Now for every Sense for every word for the original sentence we get a dictionary where the key is the Sense and value is the vector/numpy array. We calculate the number of Sense against the word and sort it by the length of Sense. \n",
    " 7. In the next step, we store this Sense and it's vector/numpy array against its actual word in a dictionary. \n",
    " 8. Now we find the length of the Sense and store in against its actual word in a different dictionary and we sort it by its length. \n",
    " 9. We initialize a context vector by the average of each word's vector. For every word in the context vector, we calculate the cosine similarity with the candidate word. We take the word which has the highest cosine similarity. We call the process as ***WSD- word sense disambiguation***. \n",
    " 10. If we can disambiguate any word we find the sense key and its name.\n",
    " 11. Now from the experiment dataset we read the text files and separate the key. With this key, we find it's corresponding sense key from the gold standard dataset. If we find it we call it as a recall. If we can not find it then it's a failure. \n",
    "> #### According to the paper, four persons had put the sentence key and several sense keys in the gold standard dataset. We counted the most common sense keys and selected one single sense key. \n",
    "  \n",
    "## Example for success\n",
    "- Let's assume we read a line from our source data file as ***\"bank.n.bnc.5425  bank ? 13 A beach of bleached stones gleamed bonewhite against the long stretch of grassy bank which rolled up to the pastures lining the valley floor . \"*** .\n",
    "- Now the code reads the line and splits the line by three parts. Which are:\n",
    "\t - ***bank.n.bnc.5425***\n",
    "\t - ***bank***\n",
    "\t - ***A beach of bleached stones gleamed bonewhite against the long stretch of grassy bank which rolled up to the pastures lining the valley floor .*** \n",
    "- With pos tagger we get below list from the sentence: \n",
    " \n",
    "\t\t    ('A', 'DT'),  ('beach', 'NN'),  ('of', 'IN'),  ('bleached', 'JJ'), \n",
    "    \t     ('stones', 'NNS'),  ('gleamed', 'VBD'),  ('bonewhite', 'RB'), \n",
    "    \t     ('against', 'IN'),  ('the', 'DT'),  ('long', 'JJ'),  ('stretch',\n",
    "    \t     'NN'),  ('of', 'IN'),  ('grassy', 'JJ'),  ('bank', 'NN'), \n",
    "    \t     ('which', 'WDT'),  ('rolled', 'VBD'),  ('up', 'RP'),  ('to', 'TO'),\n",
    "    \t     ('the', 'DT'),  ('pastures', 'NNS'),  ('lining', 'VBG'),  ('the',\n",
    "    \t     'DT'),  ('valley', 'NN'),  ('floor', 'NN'),  ('.', '.')\n",
    "- From this list we only take Noun, Verb, Adjectibe and Adverb. \n",
    "- Here we have to pick the ***bank*** word to disambiguate. We get 18 Senses for this word which are listed below. Note that for every ***Sense/Lemma*** of this list there is a list of array which we call vector. \n",
    "\t - *Lemma( bank.n.01)*\n",
    "\t - *Lemma(depository_financial_institution.n.01)*\n",
    "\t - *Lemma(bank.n.03)*\n",
    "\t - *Lemma(bank.n.04)*\n",
    "\t - *Lemma(bank.n.05)*\n",
    "\t - *Lemma(bank.n.06)*\n",
    "\t - *Lemma(bank.n.07)*\n",
    "\t - *Lemma(savings_bank.n.02)*\n",
    "\t - *Lemma(bank.n.09)*\n",
    "\t - *Lemma(bank.n.10 bank.v.01)*\n",
    "\t - *Lemma(bank.v.02)*\n",
    "\t - *Lemma(bank.v.03)*\n",
    "\t - *Lemma(bank.v.04)*\n",
    "\t - *Lemma(bank.v.05)*\n",
    "\t - *Lemma(deposit.v.02)*\n",
    "\t - *Lemma(bank.v.07)*\n",
    "\t - *Lemma(trust.v.01)*\n",
    "- If we run our code, we will see that for our example sentence and example word's (***bank***)  senses only the ***bank.n.01*** sense has the nearest sense value. \n",
    "- In our example, sense key is ***bank%1:17:01::***\n",
    "- Now from the gold standard dataset we can see that, there are three lemma keys (***bank%1:17:01::*** , ***bank%1:17:00::*** , ***bank%1:17:01::*** ) for our sentence key ***bank.n.bnc.5425***. From the sense keys we choose the most frequent key which is ***bank%1:17:01::*** . So it's a match. For the word ***bank*** and sentence key  ***bank.n.bnc.5425*** our algorithm works. \n",
    "## Example for failure \n",
    "- Now for failure example let's assume our sentence from the dataset is: ***\"bank.n.bnc.9  bank ? 12 These are not just fizzy lager joints , but environments where a bank of six beer engines can sit happily on the bar counter offering a wide selection of real ales . \"*** .\n",
    "- Now the code reads the line and splits the line by three parts. Which are:\n",
    "\t - ***bank.n.bnc.9***\n",
    "\t - ***bank***\n",
    "\t - ***These are not just fizzy lager joints , but environments where a bank of six beer engines can sit happily on the bar counter offering a wide selection of real ales .*** \n",
    "- If we repeat the steps of our algorithm, we will find that for our word ***bank*** we will find lemma key ***'bank%1:17:02::'*** from the wordnet. \n",
    "- However we will find sense key ***bank%1:14:01::*** in our gold standard dataset against our sentence key ***bank.n.bnc.9***. So they don't match. So we consider this as a failure. \n",
    "\n",
    "# Implementation\n",
    "(You can find the source code of this lab [here](https://github.com/logicalforhad/word-sense-disambiguation).)\n",
    "\n",
    "The paper describes the method of disambiguating word sense as a **3-stage** process,\n",
    "1. *Initializing word vectors and sense vectors*\n",
    "2. *Performing word sense disambiguation*\n",
    "3. *Learning sense vectors from relevant occurrences*\n",
    "\n",
    "The scope of this lab was to disambiguate the word sense. So we skipped the third stage. \n",
    "We use two python packages - `nltk` & `numpy`. \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy import average\n",
    "from numpy.linalg import norm\n",
    "```\n",
    "## Stage 1: Initializing word vectors and sense vectors\n",
    "There are two parts of this stage.\n",
    "### Initializing word vectors\n",
    "The paper uses *Skipgram* to train the word vectors from large amounts of text data. Since we use pre-trained word embeddings for word vector representation, we do not use Skipgram again to train word vectors. Below is a method to load the word vectors in a numpy array.\n",
    "```python\n",
    "def load_glove_vectors(glove_file):\n",
    "    f = open(glove_file, 'r', encoding=\"utf-8\")\n",
    "    vectors = {}\n",
    "    for line in f:\n",
    "        split_line = line.split()\n",
    "        word = split_line[0]\n",
    "        embedding = np.array([float(val) for val in split_line[1:]])\n",
    "        vectors[word] = embedding\n",
    "    f.close()\n",
    "    return vectors\n",
    "```\n",
    "This returns a dictionary of words and their corresponding vector embedding.\n",
    "\n",
    "(Our glove data can be found [here](https://nlp.stanford.edu/projects/glove/))\n",
    "### Initializing sense vectors\n",
    "As stated in the Section 2.2 of the paper, we only consider words that are either *noun*, *verb*, *adjective* or *adverb* as candidates for disambiguation. We filter out the correct parts of speech with the following method,\n",
    "\n",
    "```python\n",
    "def get_valid_pos_tag(tag):\n",
    "    if tag.startswith('J') or tag.startswith('V') or tag.startswith('N') or tag.startswith('R'):\n",
    "        return True\n",
    "    return False\n",
    "```\n",
    "Each candidate word has one or more *sense* and each sense has a *definition* and one or more *examples* in wordnet. For each sense, the task is to find its cosine similarities to each of the words in its gloss; then only take those words that clear a similarity threshold.\n",
    "```python\n",
    "cosine_sim_threshold = 0.05\n",
    "```\n",
    "We find the normalized dot product between a sense vector and a word vector in its gloss to find their cosine similarity.\n",
    "```python\n",
    "cos_sim = dot(gloss_word_vec, candidate_vec) / (norm(gloss_word_vec) * norm(candidate_vec))\n",
    "if cos_sim > cosine_sim_threshold:\n",
    "    word_vectors.append(gloss_word_vec)\n",
    "```\n",
    "Finally, the average of the vectors for these words is used as the initialization value of the sense vector.\n",
    "```python\n",
    "sense_vector = average(word_vectors, 0) # Here, 0 denotes the Axis along which to average the vector arrays\n",
    "```\n",
    "\n",
    "The method below returns all sense vectors for a candidate word -\n",
    "\n",
    "```python\n",
    "def get_word_sense_vectors(candidate):\n",
    "    vectors = {}\n",
    "    try:\n",
    "        candidate_vec = glove[candidate]\n",
    "    except Exception:\n",
    "        return None\n",
    "    for sense in wn.lemmas(candidate):\n",
    "        gloss = [sense.synset().definition()]\n",
    "        gloss.extend(sense.synset().examples())\n",
    "        word_vectors = []\n",
    "        for sentence in gloss:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            pos_tags = nltk.pos_tag(tokens)\n",
    "            for gloss_pos, tag in pos_tags:\n",
    "                if get_valid_pos_tag(tag):\n",
    "                    try:\n",
    "                        gloss_word_vec = glove[gloss_pos]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    cos_sim = dot(gloss_word_vec, candidate_vec) / (norm(gloss_word_vec) * norm(candidate_vec))\n",
    "                    if cos_sim > cosine_sim_threshold:\n",
    "                        word_vectors.append(gloss_word_vec)\n",
    "        if len(word_vectors) == 0:\n",
    "            continue\n",
    "        sense_vector = average(word_vectors, 0)\n",
    "        vectors[sense] = sense_vector\n",
    "    return vectors\n",
    "```\n",
    "## Stage 2: Performing Word Sense Disambiguation\n",
    "To disambiguate all of the content words in a sentence, we performed three steps. Which are:\n",
    " 1. *Context vector initialization*\n",
    " 2. *Ranking word by sense*\n",
    " 3. *Word sense disambiguation*\n",
    "\n",
    "### Context vector initialization\n",
    "Similar to the initialization of sense vectors, the average of all of the content wordsâ€™ vectors in a sentence as the initialization vector of context.\n",
    "```python\n",
    "context_vec = average(list(pos_vectors.values()), 0)\n",
    "```\n",
    "### Ranking word by sense\n",
    "According to the algorithm, we rank the content words based on the ascending order of their number of senses.\n",
    "```python\n",
    "sorted_sense_vectors_collection = sorted(sorted_sense_vectors_collection.items(), key=lambda x: x[1])\n",
    "```\n",
    "### Word sense disambiguation\n",
    "For each content word in the `sorted_sense_vectors_collection`, we compute the cosine similarities between its sense vectors and the context vector. We choose the sense that yields the maximum cosine similarity as its disambiguation result. We also find the cosine similarity score margin between the nearest sense (disambiguated sense) and second nearest sense. The method looks like this,\n",
    "```python\n",
    "def disambiguate_word_sense(word, context_vector):\n",
    "    vectors = sense_vectors_collection[word]\n",
    "    if len(vectors) == 0:\n",
    "        return [None, 0.0]\n",
    "    cos_sims = {}\n",
    "    for sense, sense_vector in vectors.items():\n",
    "        cos_sim = dot(context_vector, sense_vector) / (norm(context_vector) * norm(sense_vector))\n",
    "        cos_sims[sense] = cos_sim\n",
    "    sorted_list = sorted(cos_sims.items(), key=lambda x: x[1])\n",
    "    if len(sorted_list) == 0:\n",
    "        return [None, 0.0]\n",
    "    most_similar_pair = sorted_list.pop()\n",
    "    disambiguated_sense = most_similar_pair[0]\n",
    "    cos_sim_second_most_similar_sense = 0\n",
    "    if len(sorted_list) > 0:\n",
    "        cos_sim_second_most_similar_sense = sorted_list.pop()[1]\n",
    "    score_margin = most_similar_pair[1] - cos_sim_second_most_similar_sense\n",
    "    # we return the disambiguated sense AND the cosine score margin between the two most similar senses.\n",
    "    return [disambiguated_sense, score_margin]\n",
    "```\n",
    "From the above method we get the disambiguated sense and score margin. We look at the score margin and if it is greater than the threshold we defined above, we use the disambiguated sense vector to replace the word vector in `pos_vectors`. Then we repeat **Context vector initialization** to update the context vector.\n",
    "```python\n",
    "if score_margin > score_margin_threshold:\n",
    "    pos_vectors[w] = sense_vectors_collection[content_word][disambiguated_sense]\n",
    "    context_vec = average(list(pos_vectors.values()), 0)\n",
    "```\n",
    "From here, we feed the updated context vector along with the next content word to the method. This updated context vector helps better disambiguate the next content word. We repeat this entire step until all content words are disambiguated.\n",
    "# Running the experiment\n",
    "\n",
    "## Data Source\n",
    "Our dataset was taken from the following paper -\n",
    "\n",
    "**Koeling, Rob, Diana McCarthy, and John Carroll**. \"Domain-specific sense distributions and predominant sense acquisition.\" Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2005. Accessed at https://www.researchgate.net/publication/220816751_Domain-Specific_Sense_Distributions_and_Predominant_Sense_Acquisition.\n",
    "\n",
    "Dataset can be accessed [here](http://www.dianamccarthy.co.uk/downloads/hlt2005releasev2.tgz).\n",
    "\n",
    "This dataset contains 41 different content words in 3 different categories - banking & commerce, sports, and finance. For each of the 41 words, our dataset contains around 100 examples under each category. That sums to ~300 sentences for each content word to run an experiment on.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "### Loading annotations\n",
    "We start by loading the annotated WordNet sense keys from the `gold_standard_clean.txt` file in our dataset. For each sentence in the dataset, there is a linkup key provided to link its disambiguation result to the result file. For each linkup key, the dataset contains 4 sense keys annotated by 4 distinct authors. We store the sense keys against their linkup key in a dictionary.\n",
    "```python\n",
    "def load_annotations():\n",
    "    path = \"E:/Code/hlt2005releasev2/hlt2005releasev2/domainhltGS.tar/gold_standard_clean.txt\"\n",
    "    with open(path, 'r', encoding='ISO-8859-1') as f:\n",
    "        for lines in f:\n",
    "            line = lines.split('|')\n",
    "            if len(line) < 4:\n",
    "                continue\n",
    "            linkup_key = line[1].strip()\n",
    "            wn_key = line[2].strip()\n",
    "            wn_sense_keylist = list()\n",
    "            if linkup_key in annotation_results:\n",
    "                wn_sense_keylist = annotation_results[linkup_key]\n",
    "            else:\n",
    "                annotation_results[linkup_key] = wn_sense_keylist\n",
    "            if wn_key == \"unclear\":\n",
    "                continue\n",
    "            wn_sense_keylist.append(wn_key)\n",
    "```\n",
    "### Running the experiment\n",
    "We define another method takes a sentence and a lookup word as parameters and returns the key in WordNet for the disambiguated sense (if found) for the word.\n",
    "```python\n",
    "def find_wn_sense_key(sentence, lookup_word):\n",
    "```\n",
    "The next steps are to look for content words in the input sentence, find sense vectors for each content word, initializing the context vector and ranking the content words - as we've described in the **Implementation** section. After the disambiguation method returns for each content word we do the following,\n",
    "\n",
    "If the content word is the lookup word, we have found the disambiguated sense. We simply return the sense key as our disambiguation result.\n",
    "```python\n",
    "if content_word == lookup_word:\n",
    "    wn_sense_key = disambiguated_sense.key()\n",
    "    break\n",
    "return wn_sense_key\n",
    "```\n",
    "Otherwise we update the context vector based on the score margin and repeat the disambiguation process for the next content word.\n",
    "\n",
    "We match the disambiguated sense key against the most frequent sense key in `annotation_results` dictionary - linking the sentence file and dictionary through the linkup key we mentioned. If the two keys match, we consider our dismambiguation result correct.\n",
    "```python\n",
    "if most_frequent_annotated_wn_sense_key == wn_sense_key:\n",
    "    output_file.write(\"correct wsd for \" + linkup_key + \" \" + wn_sense_key + \"\\n\")\n",
    "    correct_count += 1\n",
    "    output_file.write(\"correct \" + str(correct_count) + \" | total \" + str(total_sentence_count) + \"\\n\")\n",
    "else:\n",
    "    output_file.write(\"incorrect wsd for \" + linkup_key + \" | found \" + wn_sense_key + \", correct is \" + most_frequent_annotated_wn_sense_key + \"\\n\")\n",
    "```\n",
    "From the  **correct_count** value we calculate the total recall against the dataset. \n",
    "## Results\n",
    "A summary of our results is given below:\n",
    "* Total __11427__ valid examples were executed for word sense disambiguation\n",
    "* Total __683__ invalid examples identified in gold standard clean dataset\n",
    "* Overall __12110__ examples were executed and dumped to output file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
